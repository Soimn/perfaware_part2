movnt vs mov
movnt can be beneficial when stores would pollute the cache with data that wont
be read for a very long time

for there to be a significant difference between using movnt and mov the
program has to be frequently writing to memory that only pollutes the cache,
while at the same time reading or writing to lines that will be evicted by the
pollution

only example of such a workload that comes to mind is some sort of image
upscaling routine that writes several "rows" at a time (preferably equal to
the associativity of L1)

matmul and storing the results as history?
- since the matrices being multiplied are fixed at one address they will only be evicted sometimes (once per 64K)
-> store matrices "vertically" and align all matrices to the same cache index.
   This will probably work, but is highly unrealistic, as it essentially stores
   one matrix per 64K

I am probably overthinking this

properties needed for movnt to be useful
- writing a lot of data that will not be read or written to again, "ever"
- a workload that needs most, if not all, of its working set in L1/L2 cache 
  to be remotely performant

idea of structure:
 1. compute values based on tables
 2. write out results in a way that evicts the tables
 3. repeat

simple idea:
sum N images and write M out. All images are the same dimensions and aligned to 64K. Do the sum and copy in chunks (blur?).

combine 8 images into 1 in 8 ways. input and output are consecutive 64K images

64K monochrome
(2^16)^(1/2) = 2^8
256x256 monochrome

maybe 32K instead, since D$ is only half of L1
so 256x128 images

or maybe scale it up to make the differences more apparent
1024x1024 seems like a realistic size for some sensor readout of sorts,
which is 2^20 B monochrome

The intel prefetchers seem to be very aggressive (at least the i7 8700 which has a stream prefetcher for L2 which apparently can prefetch a couple dozen lines ahead without a big startup cost), so larger working sets will probably not incurr a load of capacity misses

goal: read in the input images, and evict them, once per output image
problem: one write is not enough to cause havoc
solution: or is it

plan:
combine input images in different ways and write once to each output image. Advancing to the next pixel should then read the same cache line and cause a miss per input image, which should be bad enough. Only problem is justifying reading the input in again instead of just keeping the whole line in registers and avoiding the miss.

actual implementation:
two arrays of images, one for input and one for output
the number of input and output images can be controlled independently
operation:
 - loop over every y
  - loop over every x (every 16, since 16 xs are processed at once)
    - compute xor of all input images at x,y
    - loop over all output images
      - set output image at x,y to be the xor of all images except one
      ^ this is what is either a vmovdqu or vmovntdq

setting the image size to be 256x256, input image count to 8 and output count to 10 gave a 14% improvement from 3.5 GB/s to 4.0 GB/s. Setting  input and output count to any other values resulted in a significant performance loss

-- wacthed the beginning of the next lecture to see Casey's version
aa a shippaishita
Casey's version is significantly simpler and achieves a ~130% perf increase

but how?
Casey's version just tiles a small buffer onto a larger one
Why would movnt provide a speedup?
Because I would assume the small buffer lines would always be more recently used than all other larger buffer lines (except the one being written after)?

An illustration of what I would expect to happen (given a 4 way LRU cache with
8 slots) when the small buffer lines "s" are read first every iteration and
then some lines are written ("A", "B", "C", ...)

--- init
0 ....
1 ....
2 ....
3 ....
4 ....
5 ....
6 ....
7 ....
--- 1st iteration
0 sA..
1 sA..
2 A...
3 A...
4 A...
5 A...
6 A...
7 A...
--- 2nd iteration
0 sAB.
1 sAB.
2 AB..
3 AB..
4 AB..
5 AB..
6 AB..
7 AB..
--- 3rd iteration
0 sABC
1 sABC
2 ABC.
3 ABC.
4 ABC.
5 ABC.
6 ABC.
7 ABC.
--- 4th iteration (s,C are used in 3rd it, A last used in 1st)
0 sDBC
1 sDBC
2 ABCD
3 ABCD
4 ABCD
5 ABCD
6 ABCD
7 ABCD
--- 5th iteration (s,D are used in 4th it, B last used in 2st)
0 sDEC
1 sDEC
2 EBCD
3 EBCD
4 EBCD
5 EBCD
6 EBCD
7 EBCD

=> s is never evicted
so why does movnt result in a speedup?
As I understand it, movnt writes only to a write combining buffer (probably the
same buffer used for write back but marking the lines with a flag or something)
The point of it is to avoid populating the cache
This alleviates pressure on the cache, in exchange for suffering main memory speeds for a subsequent load of the non temporally stored memory
This can be helpful in scenarios where stores to memory that will not be read, for a long long time, pollutes the cache
Since most caches use a Least Recently Used eviction policy, useful lines will only be evicted when either all ways are filled with useful lines, or a useful line is older than a useless line. A program reading from a small buffer and writing it tiled over a larger buffer will keep reading the small buffer. When all the other lines in the same slot are pollution from writing to the larger buffer, the small buffer lines should always be accessed more recently, and therefore never evicted.

So WTF is going on!?
It would make sense if consecutive cache lines went into the same slot
RA RA ..
WA RA WA
RB RB WA
WB RB WB

RA RA ..
WA RA ..
RB RA RB
WB RA RB

be me, pingle
https://open.substack.com/pub/computerenhance/p/q-and-a-60-2024-07-22?r=1qsdcv&utm_campaign=comment-list-share-cta&utm_medium=web&comments=true&commentId=63486206
